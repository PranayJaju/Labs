{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pyspark  \n",
    "import os  \n",
    "import sys  \n",
    "```  \n",
    "Imports the `pyspark` library and the `os` and `sys` modules.  \n",
    "  \n",
    "```python  \n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable  \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable  \n",
    "```  \n",
    "Sets the `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` environment variables to the current Python executable path, ensuring that PySpark uses the same Python environment.  \n",
    "  \n",
    "```python  \n",
    "from pyspark.sql import SparkSession  \n",
    "```  \n",
    "Import SparkSession from the `pyspark.sql` module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_5').getOrCreate()  \n",
    "```  \n",
    "Creates a new spark session with 16GB memory and the name `chapter_5`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_5').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "The `column_names` list contains the names of the columns in the CSV file. These names are assigned to the columns of the DataFrame using the `toDF()` method.  \n",
    "  \n",
    "The resulting DataFrame, `data`, contains the data from the CSV file with the specified column names. This allows for easier access and manipulation of the data using the assigned column names instead of the default column names (`_c0`, `_c1`, etc.).  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\",True).option(\"header\",False).csv(\"data/kddcup.data_10_percent_corrected\")\n",
    "column_names = [ \n",
    "                \"duration\",\n",
    "                \"protocol_type\",\n",
    "                \"service\",\n",
    "                \"flag\",\n",
    "                \"src_bytes\",\n",
    "                \"dst_bytes\",\n",
    "                \"land\",\n",
    "                \"wrong_fragment\",\n",
    "                \"urgent\",\n",
    "                \"hot\",\n",
    "                \"num_failed_logins\",\n",
    "                \"logged_in\",\n",
    "                \"num_compromised\",\n",
    "                \"root_shell\",\n",
    "                \"su_attempted\",\n",
    "                \"num_root\",\n",
    "                \"num_file_creations\",\n",
    "                \"num_shells\",\n",
    "                \"num_access_files\",\n",
    "                \"num_outbound_cmds\",\n",
    "                \"is_host_login\",\n",
    "                \"is_guest_login\",\n",
    "                \"count\",\n",
    "                \"srv_count\",\n",
    "                \"serror_rate\",\n",
    "                \"srv_serror_rate\",\n",
    "                \"rerror_rate\",\n",
    "                \"srv_rerror_rate\",\n",
    "                \"same_srv_rate\",\n",
    "                \"diff_srv_rate\",\n",
    "                \"srv_diff_host_rate\",\n",
    "                \"dst_host_count\",\n",
    "                \"dst_host_srv_count\",\n",
    "                \"dst_host_same_srv_rate\",\n",
    "                \"dst_host_diff_srv_rate\",\n",
    "                \"dst_host_same_src_port_rate\",\n",
    "                \"dst_host_srv_diff_host_rate\",\n",
    "                \"dst_host_serror_rate\",\n",
    "                \"dst_host_srv_serror_rate\",\n",
    "                \"dst_host_rerror_rate\",\n",
    "                \"dst_host_srv_rerror_rate\",\n",
    "                \"label\"\n",
    "]\n",
    "data = data_without_header.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql.functions import col  \n",
    "```  \n",
    "Imports the `col` function from the `pyspark.sql.functions` module.  \n",
    "\n",
    "---\n",
    "\n",
    "`data.select(\"label\")` Selects the \"label\" column from the `data` DataFrame.  \n",
    "`.groupBy(\"label\")` Groups the selected data by the \"label\" column.  \n",
    "`.count()` Counts the number of rows for each group.  \n",
    "`.orderBy(col(\"count\").desc())` Orders the grouped and counted data in descending order based on the \"count\" column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "First, it creates a new DataFrame `numeric_only` by dropping the non-numeric columns from the original `data` DataFrame and caching it for better performance.  \n",
    "  \n",
    "Next, it sets up a `VectorAssembler` to combine the input columns (all columns except the last one) into a single feature vector column named \"featureVector\".  \n",
    "  \n",
    "It then initializes a `KMeans` object, specifying the output column for the predicted cluster (\"cluster\") and the input column for the feature vector (\"featureVector\").  \n",
    "  \n",
    "The `VectorAssembler` and `KMeans` are combined into a `Pipeline`, which is then fit on the `numeric_only` DataFrame to create a `pipeline_model`.  \n",
    "  \n",
    "The trained `KMeans` model is extracted from the `pipeline_model` and its cluster centers are printed using `pprint()`.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(kmeans_model.clusterCenters())\n",
    "\n",
    "with_cluster = pipeline_model.transform(numeric_only)\n",
    "with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql import DataFrame  \n",
    "from random import randint  \n",
    "```  \n",
    "  \n",
    "The `clustering_score` function takes an `input_data` DataFrame and a number `k` as input. It drops the columns \"protocol_type\", \"service\", and \"flag\" from the input DataFrame to create a new DataFrame `input_numeric_only` with only numeric columns.  \n",
    "  \n",
    "```python  \n",
    "input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")  \n",
    "```  \n",
    "  \n",
    "It then creates a `VectorAssembler` to combine the numeric columns (excluding the last column) into a single feature vector column named \"featureVector\".  \n",
    "  \n",
    "```python  \n",
    "assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")  \n",
    "```  \n",
    "  \n",
    "A `KMeans` model is initialized with a random seed, the specified `k` value, and the \"featureVector\" column as input. The predicted cluster for each data point will be stored in the \"cluster\" column.  \n",
    "  \n",
    "```python  \n",
    "kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")  \n",
    "```  \n",
    "  \n",
    "The `VectorAssembler` and `KMeans` are combined into a `Pipeline`, which is then fitted on the `input_numeric_only` DataFrame to create a `pipeline_model`.  \n",
    "  \n",
    "```python  \n",
    "pipeline = Pipeline().setStages([assembler, kmeans])  \n",
    "pipeline_model = pipeline.fit(input_numeric_only)  \n",
    "```  \n",
    "  \n",
    "The trained `KMeans` model is extracted from the `pipeline_model`, and its `trainingCost` (sum of squared distances of points to their nearest center) is returned as the clustering score.  \n",
    "  \n",
    "```python  \n",
    "kmeans_model = pipeline_model.stages[-1]  \n",
    "training_cost = kmeans_model.summary.trainingCost  \n",
    "return training_cost  \n",
    "```  \n",
    "  \n",
    "Finally, the script iterates over `k` values from 20 to 100 (exclusive) in steps of 20 and prints the clustering score for each `k` using the `numeric_only` DataFrame.  \n",
    "  \n",
    "```python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "\n",
    "def clustering_score(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,100, 20)):\n",
    "    print(clustering_score(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")  \n",
    "```  \n",
    "Drops the columns \"protocol_type\", \"service\", and \"flag\" from the `input_data` DataFrame and assigns the result to `input_numeric_only`.  \n",
    "  \n",
    "```python  \n",
    "assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")  \n",
    "```  \n",
    "Creates a `VectorAssembler` object that combines all columns of `input_numeric_only` except the last one into a single vector column named \"featureVector\".  \n",
    "  \n",
    "```python  \n",
    "kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")  \n",
    "```  \n",
    "Initializes a `KMeans` object with a random seed, sets the number of clusters to `k`, maximum iterations to 40, convergence tolerance to 1.0e-5, prediction column name to \"cluster\", and features column name to \"featureVector\".  \n",
    "  \n",
    "```python  \n",
    "pipeline = Pipeline().setStages([assembler, kmeans])  \n",
    "```  \n",
    "Creates a `Pipeline` object that consists of the `VectorAssembler` and `KMeans` stages.  \n",
    "  \n",
    "```python  \n",
    "pipeline_model = pipeline.fit(input_numeric_only)  \n",
    "```  \n",
    "Fits the pipeline to the `input_numeric_only` DataFrame and assigns the resulting model to `pipeline_model`.  \n",
    "  \n",
    "```python  \n",
    "kmeans_model = pipeline_model.stages[-1]  \n",
    "```  \n",
    "Extracts the trained `KMeans` model from the last stage of the pipeline.  \n",
    "  \n",
    "```python  \n",
    "training_cost = kmeans_model.summary.trainingCost  \n",
    "```  \n",
    "Retrieves the training cost from the `KMeans` model summary and assigns it to `training_cost`.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,101, 20)):\n",
    "    print(k, clustering_score_1(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "First, it removes the non-numeric columns \"protocol_type\", \"service\", and \"flag\" from the input data using `drop()`.   \n",
    "  \n",
    "Then, it creates a pipeline of three stages:  \n",
    "1. `VectorAssembler` combines the remaining columns into a single feature vector column named \"featureVector\".   \n",
    "2. `StandardScaler` standardizes the feature vectors by scaling them to unit variance, outputting the result as \"scaledFeatureVector\".   \n",
    "3. `KMeans` performs the clustering with the specified number of clusters `k`, maximum iterations, and convergence tolerance, using the scaled feature vectors. The cluster assignments are stored in the \"cluster\" column.  \n",
    "  \n",
    "The pipeline is fit to the numeric-only data, and the resulting `KMeansModel` is extracted from the pipeline.   \n",
    "  \n",
    "The training cost (sum of squared distances between points and their nearest cluster center) is obtained from the model summary and returned.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "def clustering_score_2(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(60, 271, 30)):\n",
    "    print(k, clustering_score_2(numeric_only, k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer  \n",
    "```  \n",
    "Imports the `OneHotEncoder` and `StringIndexer` classes from the `pyspark.ml.feature` module.  \n",
    "  \n",
    "```python  \n",
    "def one_hot_pipeline(input_col):  \n",
    "```  \n",
    "Defines a function `one_hot_pipeline` that takes an `input_col` parameter representing the input column name.  \n",
    "  \n",
    "```python  \n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col + \"_indexed\")  \n",
    "```  \n",
    "Creates a `StringIndexer` object that converts the string values in `input_col` to numeric indices. The output column is named `input_col + \"_indexed\"`.  \n",
    "  \n",
    "```python  \n",
    "encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\"). setOutputCol(input_col + \"_vec\")  \n",
    "```  \n",
    "Creates a `OneHotEncoder` object that performs one-hot encoding on the indexed column. The output column is named `input_col + \"_vec\"`.  \n",
    "  \n",
    "```python  \n",
    "pipeline = Pipeline().setStages([indexer, encoder])  \n",
    "```  \n",
    "Creates a `Pipeline` object and sets the stages to the `indexer` and `encoder` objects.  \n",
    "  \n",
    "```python  \n",
    "return pipeline, input_col + \"_vec\"  \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "def one_hot_pipeline(input_col):\n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col + \"_indexed\")\n",
    "    encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\"). setOutputCol(input_col + \"_vec\")\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    return pipeline, input_col + \"_vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "The pipeline consists of the following stages:  \n",
    "- `proto_type_pipeline`, `service_pipeline`, and `flag_pipeline`: One-hot encode the `protocol_type`, `service`, and `flag` columns respectively using the `one_hot_pipeline` function.  \n",
    "- `assembler`: Assembles the feature columns (excluding `label`, `protocol_type`, `service`, and `flag`) and the one-hot encoded columns into a single feature vector column named `featureVector`.  \n",
    "- `scaler`: Scales the `featureVector` using `StandardScaler` without centering (mean=0) and outputs the scaled vector as `scaledFeatureVector`.  \n",
    "- `kmeans`: Initializes the k-means model with a random seed, the specified number of clusters `k`, maximum iterations, tolerance, prediction column name, and the input features column.  \n",
    "  \n",
    "The pipeline is then fit on the `input_data` to obtain the `pipeline_model`. The k-means model is extracted from the pipeline stages, and its training cost is returned.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_3(input_data, k):\n",
    "    proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "    service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n",
    "    flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "    assemble_cols = set(input_data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "    assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline,flag_pipeline, assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_data)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "for k in list(range(60, 271, 30)):\n",
    "    print(k, clustering_score_3(data, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from math import log  \n",
    "```  \n",
    "Imports the `log` function from the built-in `math` module.  \n",
    "  \n",
    "```python  \n",
    "def entropy(counts):  \n",
    "    values = [c for c in counts if (c > 0)]\n",
    "    n = sum(values)\n",
    "    p = [v/n for v in values]\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])\n",
    "```  \n",
    "Defines a function named `entropy` that takes a list `counts` as input.  \n",
    "Creates a new list `values` containing only the positive elements from `counts`.  \n",
    "Calculates the sum of all elements in `values` and assigns it to the variable `n`.   \n",
    "Creates a new list `p` containing the normalized values of `values` by dividing each element by `n`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(counts):\n",
    "    values = [c for c in counts if (c > 0)]\n",
    "    n = sum(values)\n",
    "    p = [v/n for v in values]\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "1. `cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")`: Applies a pipeline model to the `data` DataFrame, selects the `\"cluster\"` and `\"label\"` columns, and assigns the result to `cluster_label`.  \n",
    "  \n",
    "2. `df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")`: Groups the `cluster_label` DataFrame by `\"cluster\"` and `\"label\"`, counts the occurrences of each combination, orders the result by `\"cluster\"`, and assigns it to `df`.  \n",
    "  \n",
    "3. `w = Window.partitionBy(\"cluster\")`: Creates a window partitioned by the `\"cluster\"` column.  \n",
    "  \n",
    "4. `p_col = df['count'] / fun.sum(df['count']).over(w)`: Calculates the proportion of each label within each cluster by dividing the count of each label by the total count of labels in the cluster using the window function.  \n",
    "  \n",
    "5. `with_p_col = df.withColumn(\"p_col\", p_col)`: Adds the calculated proportion column `\"p_col\"` to the `df` DataFrame.  \n",
    "  \n",
    "6. ```python  \n",
    "   result = with_p_col.groupBy(\"cluster\").agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))))  \n",
    "   .alias(\"entropy\"),  \n",
    "   fun.sum(col(\"count\"))  \n",
    "   .alias(\"cluster_size\"))  \n",
    "   ```  \n",
    "   Groups the `with_p_col` DataFrame by `\"cluster\"`, calculates the entropy and cluster size for each cluster using aggregate functions, and assigns the result to `result`.  \n",
    "  \n",
    "7. `result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))`: Adds a new column `'weightedClusterEntropy'` to the `result` DataFrame by multiplying the entropy by the cluster size.  \n",
    "  \n",
    "8. ```python  \n",
    "   weighted_cluster_entropy_avg = result.agg(fun.sum(  \n",
    "   col('weightedClusterEntropy'))).collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window\n",
    "\n",
    "cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")\n",
    "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "result = with_p_col.groupBy(\"cluster\").agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))).alias(\"entropy\"),fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))\n",
    "weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
    "weighted_cluster_entropy_avg[0][0]/data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "`fit_pipeline_4` takes `data` and `k` as parameters and performs the following steps:  \n",
    "1. Creates three pipelines (`proto_type_pipeline`, `service_pipeline`, `flag_pipeline`) using the `one_hot_pipeline` function to one-hot encode the categorical columns \"protocol_type\", \"service\", and \"flag\".  \n",
    "2. Assembles the feature columns (excluding \"label\", \"protocol_type\", \"service\", \"flag\" and including the one-hot encoded columns) into a single vector column \"featureVector\" using `VectorAssembler`.  \n",
    "3. Scales the \"featureVector\" using `StandardScaler` to create \"scaledFeatureVector\".  \n",
    "4. Applies `KMeans` clustering with random seed, specified `k`, and other parameters on the \"scaledFeatureVector\".  \n",
    "5. Combines the stages into a `Pipeline` and fits the pipeline on the input `data`.  \n",
    "  \n",
    "`clustering_score_4` takes `input_data` and `k` as parameters and calculates the weighted cluster entropy average:  \n",
    "1. Fits the pipeline using `fit_pipeline_4` on the `input_data` with the specified `k`.  \n",
    "2. Transforms the `input_data` using the fitted pipeline model and selects the \"cluster\" and \"label\" columns.  \n",
    "3. Groups the data by \"cluster\" and \"label\", counts the occurrences, and orders by \"cluster\".  \n",
    "4. Calculates the proportion of each label within each cluster using a window function and adds it as a new column \"p_col\".  \n",
    "5. Groups the data by \"cluster\" and calculates the entropy and cluster size for each cluster.  \n",
    "6. Computes the weighted cluster entropy by multiplying the entropy by the cluster size.  \n",
    "7. Calculates the weighted cluster entropy average by summing the weighted cluster entropies and dividing by the total number of data points.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_4(data, k):\n",
    "    (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
    "    (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
    "    (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
    "    assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "    assembler = VectorAssembler(inputCols=list(assemble_cols), outputCol=\"featureVector\")\n",
    "    scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
    "    kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
    "    pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "    return pipeline.fit(data)\n",
    "\n",
    "def clustering_score_4(input_data, k):\n",
    "    pipeline_model = fit_pipeline_4(input_data, k)\n",
    "    cluster_label = pipeline_model.transform(input_data).select(\"cluster\", \"label\")\n",
    "    df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "    w = Window.partitionBy(\"cluster\")\n",
    "    p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "    with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "    result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun. log2(col(\"p_col\"))).alias(\"entropy\"),\n",
    "    fun.sum(col(\"count\")). alias(\"cluster_size\"))\n",
    "    result = result.withColumn('weightedClusterEntropy', col('entropy') * col('cluster_size'))\n",
    "    weighted_cluster_entropy_avg = result.agg(fun. sum(col('weightedClusterEntropy'))).collect()\n",
    "    return weighted_cluster_entropy_avg[0][0] / input_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Next, the code transforms the `data` using the `pipeline_model` and selects the `\"cluster\"` and `\"label\"` columns. It then groups the transformed data by `\"cluster\"` and `\"label\"` and counts the number of occurrences for each combination. The result is ordered by `\"cluster\"` and `\"label\"`.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = fit_pipeline_4(data, 180)\n",
    "count_by_cluster_label = pipeline_model.transform(data).\\\n",
    "select(\"cluster\", \"label\").\\\n",
    "groupBy(\"cluster\", \"label\").\\\n",
    "count().orderBy(\"cluster\", \"label\")\n",
    "count_by_cluster_label.show("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
